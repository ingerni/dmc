{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12b39317-f54d-4d2a-8949-0f94b213253d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Objetivo del Taller 01\n",
    "Que los estudiantes pongan en práctica y refuercen lo aprendido en las sesiones 2 y 3, aplicando técnicas de ingesta de datos, validación de esquemas, manejo de errores y registro de auditoría, utilizando un dataset de origen real o simulado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0469b542-7382-4573-a33a-b2e71c284b0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1. Escoger un dataset de origen (puede ser un archivo CSV, JSON o Parquet disponible en un volumen de Unity Catalog o cargado manualmente)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "51d48be6-203e-4609-b6e6-e75c4f3c325f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Se suben los archivos a la ruta /Volumes/taller_evualado_01/default/taller01/input/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "572d658b-93cd-4909-83ba-5fe2ac11b248",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2. Diseñar y ejecutar un flujo de ingesta hacia Databricks utilizando Auto Loader según el formato y el volumen del archivo. (primero va el paso 3, luego el paso 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21cd6944-484d-46e2-b54f-24360c1714ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_compras = (\n",
    "    spark.read\n",
    "    .format(\"json\")\n",
    "    .schema(compras_schema)\n",
    "    .load(\"/Volumes/taller_evualado_01/default/taller01/input/\")\n",
    "    .withColumn(\"archivo_origen\", col(\"_metadata.file_path\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "0fdaf28c-d5aa-4005-91b4-646d80e87397",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_compras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bde4696e-a5aa-4d02-a019-13f0db93370a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3. Definir un esquema explícito con StructType y aplicarlo durante la carga."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "865f7321-6028-4b7f-930e-e35874338370",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StringType, IntegerType, DoubleType, TimestampType\n",
    "\n",
    "compras_schema = (\n",
    "    StructType()\n",
    "    .add(\"compra_id\", StringType())\n",
    "    .add(\"cliente_id\", StringType())\n",
    "    .add(\"producto\", StringType())\n",
    "    .add(\"cantidad\", IntegerType())\n",
    "    .add(\"precio_unitario\", DoubleType())\n",
    "    .add(\"fecha_compra\", TimestampType())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45ddb28c-9991-4217-937b-00df9611c011",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4. Configurar un badRecordsPath y capturar información de registros inválidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac6d2ae1-9bf9-4e18-b57d-da57d44a314e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StringType, IntegerType, DoubleType, TimestampType\n",
    "\n",
    "compras_schema = (\n",
    "    StructType()\n",
    "    .add(\"compra_id\", StringType())\n",
    "    .add(\"cliente_id\", StringType())\n",
    "    .add(\"producto\", StringType())\n",
    "    .add(\"cantidad\", IntegerType())\n",
    "    .add(\"precio_unitario\", DoubleType())\n",
    "    .add(\"fecha_compra\", TimestampType())\n",
    "    .add(\"_rescued_data\", StringType())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fa01649-65ce-48ef-bbf6-4e20d7ee7264",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_compras = (\n",
    "    spark.read\n",
    "    .format(\"json\")\n",
    "    .schema(compras_schema)\n",
    "    .option(\"columnNameOfCorruptRecord\", \"_rescued_data\")\n",
    "    #.option(\"badRecordsPath\", \"/Volumes/taller_evualado_01/default/taller01/bad_records/\") # _rescued_data se almacena en un directorio\n",
    "    .load(\"/Volumes/taller_evualado_01/default/taller01/input/\")\n",
    "    .withColumn(\"archivo_origen\", col(\"_metadata.file_path\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "656da3b1-c5f2-41fe-8117-080dab1833b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_compras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73df4429-8d6a-4510-bb9d-5e44ed8cc312",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 5. Separar datos válidos e inválidos en tablas Delta diferentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e52daffc-de9b-46b2-89dd-9cbad0156816",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "try:\n",
    "    df_validos = df_compras.filter(col(\"_rescued_data\").isNull())\n",
    "    df_invalidos = df_compras.filter(col(\"_rescued_data\").isNotNull())\n",
    "\n",
    "    df_validos.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"taller_evualado_01.default.compras_bien\")\n",
    "    df_invalidos.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"taller_evualado_01.default.compras_mal\")\n",
    "\n",
    "    print(\"Escritura realizada con éxito.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error durante la escritura: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "e0bc6af6-be13-408b-a77d-6b62a7a46084",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql  select * from taller_evualado_01.default.compras_bien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f13482f7-655b-40f5-bd7d-899f7b09e2c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql  select * from taller_evualado_01.default.compras_mal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0842b52e-db8d-45c5-ba08-72e952500c40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 6. Agregar una columna de auditoría con la ruta de archivo de origen (_metadata.file_path)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a7ed20b-19c0-4141-a767-881da9538bc8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_compras = (\n",
    "    spark.read\n",
    "    .format(\"json\")\n",
    "    .schema(compras_schema)\n",
    "    .option(\"columnNameOfCorruptRecord\", \"_rescued_data\")\n",
    "    #.option(\"badRecordsPath\", \"/Volumes/taller_evualado_01/default/taller01/bad_records/\") # _rescued_data se almacena en un directorio\n",
    "    .load(\"/Volumes/taller_evualado_01/default/taller01/input/\")\n",
    "    .withColumn(\"auditoria\", col(\"_metadata.file_path\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f709c05-9260-4afd-b9d9-e4f02fc16883",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_compras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e9eca5f9-bf59-4e52-a2be-09e73f92139b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 7. Guardar los registros inválidos en una tabla Delta de auditoría usando saveAsTable()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e363d46-dee3-456d-8664-c6cd0f552a12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_invalidos = df_compras.filter(col(\"_rescued_data\").isNotNull())\n",
    "df_invalidos.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"taller_evualado_01.default.auditoria_compras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58ee461c-1517-40f5-9c05-fc9de1047141",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql  select * from taller_evualado_01.default.auditoria_compras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53af5023-93e2-4f48-86e7-36fe51772364",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 8. Documentar las decisiones tomadas, las dificultades encontradas y los resultados obtenidos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f17e3fd5-a4a7-4729-a522-6e1200d1bf00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Desiciones Tomadas\n",
    "   \n",
    "    a. Se definio la data a ingestar y luego usar la opcion de read y no readstream\n",
    "    \n",
    "    b. usar las clases para dar forma al notebook\n",
    "\n",
    "2. Dificultades encontradas\n",
    "    \n",
    "    a. una dificultad mia fue el conocer poco el tema de databricks\n",
    "\n",
    "3. Resultados Obtenidos\n",
    "    \n",
    "    a. aprender mas sobre el uso de databricks\n",
    "   \n",
    "    b. aprender sobre la ingesta de datos\n",
    "    \n",
    "    c. aprender a separar los registros malos de los buenos\n",
    "   \n",
    "    d. guardar en tablas distinas los registros buenos de los malos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d39b4d0-eaa3-406a-9779-14da12185f9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Imagenes de las tablas Delta\n",
    "\n",
    "![](/Volumes/taller_evualado_01/default/taller01/images/tablas delta.jpg)\n",
    "![](/Volumes/taller_evualado_01/default/taller01/images/tabla_compras_bien.jpg)\n",
    "![](/Volumes/taller_evualado_01/default/taller01/images/tabla_compras_mal.jpg)\n",
    "![](/Volumes/taller_evualado_01/default/taller01/images/tabla_auditoria_compras.jpg)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": "HIGH"
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5371841572301197,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "TallerEvaluado01",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
